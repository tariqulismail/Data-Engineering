

# Customer 360 Data Integration Project

## ğŸ“‹ Overview
This project implements a comprehensive Customer 360 Data Integration solution that extracts, transforms, and loads customer-related data from various sources into a centralized data warehouse (Amazon Redshift). The goal is to create a unified customer view for analytics and business intelligence dashboards.

![Customer 360 Data Integration Mindmap](https://cdn1.genspark.ai/user-upload-image/imagen_generated/80467853-7a20-42be-bea0-fa73e9b2efcd)

## ğŸ—ï¸ Architecture

The solution follows a modern data architecture with the following components:

- **Data Sources**: Multiple customer data touchpoints
- **Ingestion Layer**: Apache Airflow for orchestration
- **Staging Area**: Amazon S3 for raw data storage
- **Transformation Layer**: dbt for data modeling and transformation
- **Serving Layer**: Amazon Redshift as the data warehouse
- **BI Layer**: Power BI/Tableau for visualization and analytics

## ğŸ”„ Data Pipeline

### Data Sources
- **CRM Data** (MySQL)
  - Generated with Faker library
  - Contains customer profile information
- **Salesforce Data**
  - Extracted via simple-salesforce Python package
  - Captures sales interactions and opportunities
- **Marketing Data** (Google Analytics)
  - BigQuery public datasets: `bigquery-public-data.google_analytics_sample.ga_sessions_*`
  - Provides digital customer journey data
- **Offline Transactions** (CSV Files)
  - Generated with Mockaroo
  - Contains in-store/offline purchase information

### ETL Process
1. **Extract**: Airflow DAGs pull data from each source
2. **Load**: Raw data stored in S3 buckets
3. **Transform**: dbt models clean, standardize, and join data
4. **Validate**: great_expectations library ensures data quality
5. **Load**: Final models deployed to Amazon Redshift REPORTING schema

## ğŸ› ï¸ Technologies

- **Orchestration**: Apache Airflow
- **Storage**: Amazon S3, Amazon Redshift
- **Transformation**: dbt (data build tool)
- **Data Validation**: great_expectations
- **Visualization**: Power BI/Tableau
- **Infrastructure**: Docker, GitHub Actions

## ğŸ“¦ Project Structure

```
customer_360/
â”‚
â”œâ”€â”€ README.md                     # Project documentation
â”‚
â”œâ”€â”€ architecture/
â”‚   â””â”€â”€ architecture_diagram.png  # High-level architecture visualization
â”‚
â”œâ”€â”€ dags/                         # Airflow DAG definitions
â”‚   â”œâ”€â”€ crm_extract.py            # MySQL CRM data extraction
â”‚   â”œâ”€â”€ salesforce_extract.py     # Salesforce data extraction
â”‚   â”œâ”€â”€ ga_extract.py             # Google Analytics extraction
â”‚   â””â”€â”€ offline_extract.py        # CSV offline transaction extraction
â”‚
â”œâ”€â”€ mock_data/                    # Data generation scripts
â”‚   â”œâ”€â”€ generate_crm_data.py      # Faker script for CRM data
â”‚   â””â”€â”€ generate_transactions.py  # Mockaroo script for transaction data
â”‚
â”œâ”€â”€ models/                       # dbt models
â”‚   â”œâ”€â”€ dbt_project.yml           # dbt project configuration
â”‚   â”œâ”€â”€ sources.yml               # Data source definitions
â”‚   â”œâ”€â”€ staging/                  # Staging models
â”‚   â”‚   â”œâ”€â”€ stg_crm.sql
â”‚   â”‚   â”œâ”€â”€ stg_salesforce.sql
â”‚   â”‚   â”œâ”€â”€ stg_ga.sql
â”‚   â”‚   â””â”€â”€ stg_transactions.sql
â”‚   â”œâ”€â”€ intermediate/             # Intermediate models
â”‚   â”‚   â”œâ”€â”€ int_customer_profile.sql
â”‚   â”‚   â””â”€â”€ int_customer_activity.sql
â”‚   â””â”€â”€ marts/                    # Final dimensional models
â”‚       â”œâ”€â”€ dim_customer.sql
â”‚       â”œâ”€â”€ fct_transactions.sql
â”‚       â””â”€â”€ customer_360.sql
â”‚
â”œâ”€â”€ validation/                   # Data validation
â”‚   â”œâ”€â”€ expectations/             # great_expectations configurations
â”‚   â””â”€â”€ validate_data.py          # Validation script
â”‚
â”œâ”€â”€ sql_scripts/                  # Helper SQL scripts
â”‚   â”œâ”€â”€ create_schemas.sql        # Schema creation scripts
â”‚   â””â”€â”€ reporting_views.sql       # View definitions for reporting
â”‚
â”œâ”€â”€ docker/                       # Docker configuration
â”‚   â”œâ”€â”€ Dockerfile                # Base image definition
â”‚   â””â”€â”€ docker-compose.yml        # Service configuration
â”‚
â””â”€â”€ .github/                      # CI/CD configuration
    â””â”€â”€ workflows/
        â”œâ”€â”€ dbt_test.yml          # Workflow for dbt tests
        â””â”€â”€ deploy.yml            # Deployment workflow
```

## âš™ï¸ Setup and Installation

### Prerequisites
- Docker and Docker Compose
- AWS Account with S3 and Redshift access
- Salesforce Developer Account
- Python 3.8+

### Getting Started

1. **Clone the repository**
```bash
git clone https://github.com/yourusername/customer_360.git
cd customer_360
```

2. **Set up environment variables**
```bash
cp .env.example .env
# Edit .env with your credentials
```

3. **Start the local environment**
```bash
docker-compose up -d
```

4. **Initialize Airflow connections**
```bash
docker-compose exec airflow airflow connections add 'aws_default' \
    --conn-type 'aws' \
    --conn-login '<your-access-key>' \
    --conn-password '<your-secret-key>' \
    --conn-extra '{"region_name": "us-east-1"}'
```

5. **Initialize dbt**
```bash
cd models
dbt deps
dbt seed
```

## ğŸ“Š Data Models

### Core Models

#### Customer 360 Unified Profile
- Combines data from all sources into a single customer view
- Includes demographic, behavioral, and transactional data
- Provides a 360-degree view of the customer journey

#### Customer Engagement Metrics
- Tracks customer interactions across channels
- Calculates engagement scores and activity levels
- Identifies preferred communication channels

#### Channel Attribution
- Attributes conversions to marketing channels
- Provides insight into the customer acquisition journey
- Helps optimize marketing spend

## ğŸ” Monitoring & CI/CD

- **GitHub Actions**: Automated testing and deployment
- **Docker**: Containerized environment for consistent execution
- **dbt tests**: Data quality and integrity validation
- **Airflow monitoring**: DAG execution tracking and alerts

## ğŸ¤ Contributing

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ“ Contact

If you have any questions or want to contribute, please reach out!

---

*This README is part of the Customer 360 Data Integration Project*
