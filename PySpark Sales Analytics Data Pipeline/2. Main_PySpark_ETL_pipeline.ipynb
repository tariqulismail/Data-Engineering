{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4187da3b-62c6-4d8a-a59c-5960d6a3f399",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Sales Analytics Data Pipeline - ETL with PySpark and Star Schema Modeling\n",
    "##Author: Tariqul Ismail, Data Engineering Team Lead\n",
    "##Description: Complete ETL/ELT pipeline for transforming raw sales data into a star schema model\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import sum as spark_sum\n",
    "import logging\n",
    "import builtins\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class SalesDataETL:\n",
    "    def __init__(self, app_name=\"SalesAnalyticsETL\"):\n",
    "        \"\"\"Initialize Spark session and configuration\"\"\"\n",
    "        self.spark = SparkSession.builder \\\n",
    "            .appName(app_name) \\\n",
    "            .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "            .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "            .config(\"spark.sql.warehouse.dir\", \"/tmp/spark-warehouse\") \\\n",
    "            .getOrCreate()\n",
    "        \n",
    "        # Set log level to reduce noise\n",
    "        self.spark.sparkContext.setLogLevel(\"WARN\")\n",
    "        \n",
    "        logger.info(f\"Spark session initialized: {app_name}\")\n",
    "    \n",
    "    def extract_data(self, data_path):\n",
    "        \"\"\"Extract data from CSV files with schema validation\"\"\"\n",
    "        logger.info(\"Starting data extraction...\")\n",
    "        \n",
    "        try:\n",
    "            # Define schemas for better performance and data quality\n",
    "            sales_schema = StructType([\n",
    "                StructField(\"transaction_id\", StringType(), False),\n",
    "                StructField(\"customer_id\", StringType(), False),\n",
    "                StructField(\"product_id\", StringType(), False),\n",
    "                StructField(\"location_id\", StringType(), False),\n",
    "                StructField(\"transaction_date\", DateType(), False),\n",
    "                StructField(\"quantity\", IntegerType(), False),\n",
    "                StructField(\"unit_price\", DoubleType(), False),\n",
    "                StructField(\"gross_amount\", DoubleType(), False),\n",
    "                StructField(\"discount_amount\", DoubleType(), True),\n",
    "                StructField(\"net_amount\", DoubleType(), False),\n",
    "                StructField(\"channel\", StringType(), True),\n",
    "                StructField(\"payment_method\", StringType(), True),\n",
    "                StructField(\"sales_rep_id\", StringType(), True),\n",
    "                StructField(\"promo_code\", StringType(), True)\n",
    "            ])\n",
    "            \n",
    "            # Read CSV files\n",
    "            self.raw_sales = self.spark.read \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .option(\"inferSchema\", \"false\") \\\n",
    "                .schema(sales_schema) \\\n",
    "                .csv(f\"{data_path}/sales.csv\")\n",
    "            \n",
    "            self.raw_customers = self.spark.read \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .option(\"inferSchema\", \"true\") \\\n",
    "                .csv(f\"{data_path}/customers.csv\")\n",
    "            \n",
    "            self.raw_products = self.spark.read \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .option(\"inferSchema\", \"true\") \\\n",
    "                .csv(f\"{data_path}/products.csv\")\n",
    "            \n",
    "            self.raw_locations = self.spark.read \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .option(\"inferSchema\", \"true\") \\\n",
    "                .csv(f\"{data_path}/locations.csv\")\n",
    "            \n",
    "            # Cache frequently used datasets\n",
    "            self.raw_sales.cache()\n",
    "            self.raw_customers.cache()\n",
    "            self.raw_products.cache()\n",
    "            self.raw_locations.cache()\n",
    "            \n",
    "            logger.info(f\"Data extraction completed:\")\n",
    "            logger.info(f\"  - Sales: {self.raw_sales.count():,} records\")\n",
    "            logger.info(f\"  - Customers: {self.raw_customers.count():,} records\")\n",
    "            logger.info(f\"  - Products: {self.raw_products.count():,} records\")\n",
    "            logger.info(f\"  - Locations: {self.raw_locations.count():,} records\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during data extraction: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def transform_data(self):\n",
    "        \"\"\"Apply comprehensive data transformations\"\"\"\n",
    "        logger.info(\"Starting data transformation...\")\n",
    "        \n",
    "        try:\n",
    "            # 1. Clean Sales Data\n",
    "            self.clean_sales = self._clean_sales_data()\n",
    "            \n",
    "            # 2. Clean Dimension Tables\n",
    "            self.clean_customers = self._clean_customers_data()\n",
    "            self.clean_products = self._clean_products_data()\n",
    "            self.clean_locations = self._clean_locations_data()\n",
    "            \n",
    "            # 3. Create Date Dimension\n",
    "            self.dim_dates = self._create_date_dimension()\n",
    "            \n",
    "            # 4. Create Star Schema Tables\n",
    "            self.fact_sales = self._create_fact_sales()\n",
    "            self.dim_customers_final = self._create_dim_customers()\n",
    "            self.dim_products_final = self._create_dim_products()\n",
    "            self.dim_locations_final = self._create_dim_locations()\n",
    "            \n",
    "            logger.info(\"Data transformation completed successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during data transformation: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def _clean_sales_data(self):\n",
    "        \"\"\"Clean and validate sales transaction data\"\"\"\n",
    "        logger.info(\"Cleaning sales data...\")\n",
    "        \n",
    "        # Remove duplicates based on business logic\n",
    "        window_spec = Window.partitionBy(\"customer_id\", \"product_id\", \"location_id\", \"transaction_date\", \"net_amount\").orderBy(\"transaction_date\")\n",
    "        \n",
    "        cleaned_sales = self.raw_sales \\\n",
    "            .withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
    "            .filter(col(\"row_num\") == 1) \\\n",
    "            .drop(\"row_num\")\n",
    "        \n",
    "        # Data quality checks and corrections\n",
    "        cleaned_sales = cleaned_sales \\\n",
    "            .filter(col(\"transaction_id\").isNotNull()) \\\n",
    "            .filter(col(\"customer_id\").isNotNull()) \\\n",
    "            .filter(col(\"product_id\").isNotNull()) \\\n",
    "            .filter(col(\"location_id\").isNotNull()) \\\n",
    "            .filter(col(\"quantity\") > 0) \\\n",
    "            .filter(col(\"unit_price\") > 0) \\\n",
    "            .filter(col(\"net_amount\") >= 0) \\\n",
    "            .withColumn(\"discount_amount\", \n",
    "                       when(col(\"discount_amount\").isNull(), 0.0).otherwise(col(\"discount_amount\"))) \\\n",
    "            .withColumn(\"revenue\", col(\"net_amount\")) \\\n",
    "            .withColumn(\"profit\", col(\"net_amount\") - (col(\"quantity\") * col(\"unit_price\") * 0.6)) \\\n",
    "            .withColumn(\"year\", year(col(\"transaction_date\"))) \\\n",
    "            .withColumn(\"month\", month(col(\"transaction_date\"))) \\\n",
    "            .withColumn(\"day\", dayofmonth(col(\"transaction_date\"))) \\\n",
    "            .withColumn(\"quarter\", quarter(col(\"transaction_date\"))) \\\n",
    "            .withColumn(\"day_of_week\", dayofweek(col(\"transaction_date\"))) \\\n",
    "            .withColumn(\"is_weekend\", when(col(\"day_of_week\").isin([1, 7]), True).otherwise(False))\n",
    "        \n",
    "        logger.info(f\"Sales data cleaned: {cleaned_sales.count():,} records after deduplication and validation\")\n",
    "        return cleaned_sales\n",
    "    \n",
    "    def _clean_customers_data(self):\n",
    "        \"\"\"Clean customer data with deduplication\"\"\"\n",
    "        logger.info(\"Cleaning customer data...\")\n",
    "        \n",
    "        # Deduplicate customers based on email\n",
    "        window_spec = Window.partitionBy(\"email\").orderBy(desc(\"registration_date\"))\n",
    "        \n",
    "        cleaned_customers = self.raw_customers \\\n",
    "            .withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
    "            .filter(col(\"row_num\") == 1) \\\n",
    "            .drop(\"row_num\") \\\n",
    "            .filter(col(\"customer_id\").isNotNull()) \\\n",
    "            .filter(col(\"email\").isNotNull()) \\\n",
    "            .withColumn(\"full_name\", concat(col(\"first_name\"), lit(\" \"), col(\"last_name\"))) \\\n",
    "            .withColumn(\"customer_age\", \n",
    "                       floor(datediff(current_date(), col(\"date_of_birth\")) / 365.25))\n",
    "        \n",
    "        logger.info(f\"Customer data cleaned: {cleaned_customers.count():,} records\")\n",
    "        return cleaned_customers\n",
    "    \n",
    "    def _clean_products_data(self):\n",
    "        \"\"\"Clean product data\"\"\"\n",
    "        logger.info(\"Cleaning product data...\")\n",
    "        \n",
    "        cleaned_products = self.raw_products \\\n",
    "            .filter(col(\"product_id\").isNotNull()) \\\n",
    "            .filter(col(\"unit_price\") > 0) \\\n",
    "            .withColumn(\"profit_margin\", \n",
    "                       (col(\"unit_price\") - col(\"cost_price\")) / col(\"unit_price\")) \\\n",
    "            .withColumn(\"is_active\", when(col(\"status\") == \"Active\", True).otherwise(False))\n",
    "        \n",
    "        logger.info(f\"Product data cleaned: {cleaned_products.count():,} records\")\n",
    "        return cleaned_products\n",
    "    \n",
    "    def _clean_locations_data(self):\n",
    "        \"\"\"Clean location data\"\"\"\n",
    "        logger.info(\"Cleaning location data...\")\n",
    "        \n",
    "        cleaned_locations = self.raw_locations \\\n",
    "            .filter(col(\"location_id\").isNotNull()) \\\n",
    "            .withColumn(\"store_size_category\",\n",
    "                       when(col(\"store_size_sqft\") < 5000, \"Small\")\n",
    "                       .when(col(\"store_size_sqft\") < 20000, \"Medium\")\n",
    "                       .otherwise(\"Large\"))\n",
    "        \n",
    "        logger.info(f\"Location data cleaned: {cleaned_locations.count():,} records\")\n",
    "        return cleaned_locations\n",
    "    \n",
    "    def _create_date_dimension(self):\n",
    "        \"\"\"Create comprehensive date dimension table\"\"\"\n",
    "        logger.info(\"Creating date dimension...\")\n",
    "        \n",
    "        # Get date range from sales data\n",
    "        date_range = self.clean_sales.select(\n",
    "            min(\"transaction_date\").alias(\"min_date\"),\n",
    "            max(\"transaction_date\").alias(\"max_date\")\n",
    "        ).collect()[0]\n",
    "        \n",
    "        # Create date range\n",
    "        from datetime import datetime, timedelta\n",
    "        \n",
    "        start_date = date_range[\"min_date\"]\n",
    "        end_date = date_range[\"max_date\"]\n",
    "        \n",
    "        # Generate date range using Spark SQL\n",
    "        self.spark.sql(f\"\"\"\n",
    "            SELECT explode(sequence(to_date('{start_date}'), to_date('{end_date}'), interval 1 day)) as date_value\n",
    "        \"\"\").createOrReplaceTempView(\"date_range\")\n",
    "        \n",
    "        dim_dates = self.spark.sql(\"\"\"\n",
    "            SELECT \n",
    "                date_format(date_value, 'yyyyMMdd') as date_id,\n",
    "                date_value as full_date,\n",
    "                year(date_value) as year,\n",
    "                month(date_value) as month,\n",
    "                dayofmonth(date_value) as day,\n",
    "                quarter(date_value) as quarter,\n",
    "                dayofweek(date_value) as day_of_week,\n",
    "                date_format(date_value, 'EEEE') as day_name,\n",
    "                date_format(date_value, 'MMMM') as month_name,\n",
    "                case when dayofweek(date_value) in (1, 7) then true else false end as is_weekend,\n",
    "                case when month(date_value) in (12, 1, 2) then 'Winter'\n",
    "                     when month(date_value) in (3, 4, 5) then 'Spring'\n",
    "                     when month(date_value) in (6, 7, 8) then 'Summer'\n",
    "                     else 'Fall' end as season,\n",
    "                weekofyear(date_value) as week_of_year\n",
    "            FROM date_range\n",
    "        \"\"\")\n",
    "        \n",
    "        logger.info(f\"Date dimension created: {dim_dates.count():,} records\")\n",
    "        return dim_dates\n",
    "    \n",
    "    def _create_fact_sales(self):\n",
    "        \"\"\"Create fact sales table with all measures\"\"\"\n",
    "        logger.info(\"Creating fact sales table...\")\n",
    "        \n",
    "        fact_sales = self.clean_sales \\\n",
    "            .select(\n",
    "                col(\"transaction_id\"),\n",
    "                col(\"customer_id\"),\n",
    "                col(\"product_id\"),\n",
    "                col(\"location_id\"),\n",
    "                date_format(col(\"transaction_date\"), \"yyyyMMdd\").alias(\"date_id\"),\n",
    "                col(\"quantity\"),\n",
    "                col(\"unit_price\"),\n",
    "                col(\"gross_amount\"),\n",
    "                col(\"discount_amount\"),\n",
    "                col(\"net_amount\").alias(\"revenue\"),\n",
    "                col(\"profit\"),\n",
    "                col(\"channel\"),\n",
    "                col(\"payment_method\"),\n",
    "                col(\"sales_rep_id\"),\n",
    "                col(\"promo_code\"),\n",
    "                current_timestamp().alias(\"load_timestamp\")\n",
    "            )\n",
    "        \n",
    "        logger.info(f\"Fact sales table created: {fact_sales.count():,} records\")\n",
    "        return fact_sales\n",
    "    \n",
    "    def _create_dim_customers(self):\n",
    "        \"\"\"Create customer dimension table\"\"\"\n",
    "        logger.info(\"Creating customer dimension...\")\n",
    "        \n",
    "        dim_customers = self.clean_customers \\\n",
    "            .select(\n",
    "                col(\"customer_id\"),\n",
    "                col(\"full_name\"),\n",
    "                col(\"first_name\"),\n",
    "                col(\"last_name\"),\n",
    "                col(\"email\"),\n",
    "                col(\"phone\"),\n",
    "                col(\"customer_age\"),\n",
    "                col(\"gender\"),\n",
    "                col(\"age_group\"),\n",
    "                col(\"customer_segment\"),\n",
    "                col(\"registration_date\"),\n",
    "                col(\"preferred_contact\"),\n",
    "                col(\"loyalty_points\"),\n",
    "                col(\"city\"),\n",
    "                col(\"state\"),\n",
    "                col(\"zip_code\"),\n",
    "                current_timestamp().alias(\"load_timestamp\")\n",
    "            )\n",
    "        \n",
    "        logger.info(f\"Customer dimension created: {dim_customers.count():,} records\")\n",
    "        return dim_customers\n",
    "    \n",
    "    def _create_dim_products(self):\n",
    "        \"\"\"Create product dimension table\"\"\"\n",
    "        logger.info(\"Creating product dimension...\")\n",
    "        \n",
    "        dim_products = self.clean_products \\\n",
    "            .select(\n",
    "                col(\"product_id\"),\n",
    "                col(\"product_name\"),\n",
    "                col(\"category\"),\n",
    "                col(\"subcategory\"),\n",
    "                col(\"brand\"),\n",
    "                col(\"unit_price\"),\n",
    "                col(\"cost_price\"),\n",
    "                col(\"profit_margin\"),\n",
    "                col(\"weight_lbs\"),\n",
    "                col(\"launch_date\"),\n",
    "                col(\"is_active\"),\n",
    "                current_timestamp().alias(\"load_timestamp\")\n",
    "            )\n",
    "        \n",
    "        logger.info(f\"Product dimension created: {dim_products.count():,} records\")\n",
    "        return dim_products\n",
    "    \n",
    "    def _create_dim_locations(self):\n",
    "        \"\"\"Create location dimension table\"\"\"\n",
    "        logger.info(\"Creating location dimension...\")\n",
    "        \n",
    "        dim_locations = self.clean_locations \\\n",
    "            .select(\n",
    "                col(\"location_id\"),\n",
    "                col(\"city\"),\n",
    "                col(\"state\"),\n",
    "                col(\"region\"),\n",
    "                col(\"country\"),\n",
    "                col(\"zip_code\"),\n",
    "                col(\"store_type\"),\n",
    "                col(\"store_size_sqft\"),\n",
    "                col(\"store_size_category\"),\n",
    "                col(\"opening_date\"),\n",
    "                current_timestamp().alias(\"load_timestamp\")\n",
    "            )\n",
    "        \n",
    "        logger.info(f\"Location dimension created: {dim_locations.count():,} records\")\n",
    "        return dim_locations\n",
    "    \n",
    "    def validate_data(self):\n",
    "        \"\"\"Comprehensive data validation and quality checks\"\"\"\n",
    "        logger.info(\"Starting data validation...\")\n",
    "        \n",
    "        validation_results = {}\n",
    "        \n",
    "        try:\n",
    "            # 1. Record count validation\n",
    "            fact_count = self.fact_sales.count()\n",
    "            validation_results['fact_sales_count'] = fact_count\n",
    "            \n",
    "            # 2. Null value checks in foreign keys\n",
    "            null_customer_ids = self.fact_sales.filter(col(\"customer_id\").isNull()).count()\n",
    "            null_product_ids = self.fact_sales.filter(col(\"product_id\").isNull()).count()\n",
    "            null_location_ids = self.fact_sales.filter(col(\"location_id\").isNull()).count()\n",
    "            null_date_ids = self.fact_sales.filter(col(\"date_id\").isNull()).count()\n",
    "            \n",
    "            validation_results['null_foreign_keys'] = {\n",
    "                'customer_id': null_customer_ids,\n",
    "                'product_id': null_product_ids,\n",
    "                'location_id': null_location_ids,\n",
    "                'date_id': null_date_ids\n",
    "            }\n",
    "            \n",
    "            # 3. Business logic validation\n",
    "            negative_revenue = self.fact_sales.filter(col(\"revenue\") < 0).count()\n",
    "            invalid_quantities = self.fact_sales.filter(col(\"quantity\") <= 0).count()\n",
    "            \n",
    "            validation_results['business_rules'] = {\n",
    "                'negative_revenue': negative_revenue,\n",
    "                'invalid_quantities': invalid_quantities\n",
    "            }\n",
    "            \n",
    "            # 4. Referential integrity checks\n",
    "            orphan_customers = self.fact_sales.join(\n",
    "                self.dim_customers_final, \"customer_id\", \"left_anti\"\n",
    "            ).count()\n",
    "            \n",
    "            orphan_products = self.fact_sales.join(\n",
    "                self.dim_products_final, \"product_id\", \"left_anti\"\n",
    "            ).count()\n",
    "            \n",
    "            orphan_locations = self.fact_sales.join(\n",
    "                self.dim_locations_final, \"location_id\", \"left_anti\"\n",
    "            ).count()\n",
    "            \n",
    "            validation_results['referential_integrity'] = {\n",
    "                'orphan_customers': orphan_customers,\n",
    "                'orphan_products': orphan_products,\n",
    "                'orphan_locations': orphan_locations\n",
    "            }\n",
    "            \n",
    "            # 5. Data distribution checks\n",
    "            date_range_check = self.fact_sales.select(\n",
    "                min(\"date_id\").alias(\"min_date\"),\n",
    "                max(\"date_id\").alias(\"max_date\")\n",
    "            ).collect()[0]\n",
    "            \n",
    "            validation_results['date_range'] = {\n",
    "                'min_date': date_range_check['min_date'],\n",
    "                'max_date': date_range_check['max_date']\n",
    "            }\n",
    "            \n",
    "            # Print validation summary\n",
    "            logger.info(\"=== DATA VALIDATION SUMMARY ===\")\n",
    "            logger.info(f\"Total fact records: {fact_count:,}\")\n",
    "            #logger.info(f\"Null foreign keys: {sum(validation_results['null_foreign_keys'].values())}\")\n",
    "            logger.info(f\"Null foreign keys: {builtins.sum(validation_results['null_foreign_keys'].values())}\")\n",
    "            #logger.info(f\"Business rule violations: {sum(validation_results['business_rules'].values())}\")\n",
    "            logger.info(f\"Business rule violations: {builtins.sum(validation_results['business_rules'].values())}\")\n",
    "            #logger.info(f\"Referential integrity issues: {sum(validation_results['referential_integrity'].values())}\")\n",
    "            logger.info(f\"Referential integrity issues: {builtins.sum(validation_results['referential_integrity'].values())}\")\n",
    "            logger.info(f\"Date range: {validation_results['date_range']['min_date']} to {validation_results['date_range']['max_date']}\")\n",
    "            \n",
    "            # Assert critical validations\n",
    "            assert null_customer_ids == 0, f\"Found {null_customer_ids} null customer IDs\"\n",
    "            assert null_product_ids == 0, f\"Found {null_product_ids} null product IDs\"\n",
    "            assert null_location_ids == 0, f\"Found {null_location_ids} null location IDs\"\n",
    "            assert negative_revenue == 0, f\"Found {negative_revenue} negative revenue records\"\n",
    "            assert invalid_quantities == 0, f\"Found {invalid_quantities} invalid quantity records\"\n",
    "            \n",
    "            logger.info(\"✓ All critical validations passed\")\n",
    "            \n",
    "        except AssertionError as e:\n",
    "            logger.error(f\"Validation failed: {str(e)}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during validation: {str(e)}\")\n",
    "            raise\n",
    "        \n",
    "        return validation_results\n",
    "    \n",
    "    def save_data(self, output_path, format=\"parquet\"):\n",
    "        \"\"\"Save transformed data to specified format with partitioning\"\"\"\n",
    "        logger.info(f\"Saving data to {output_path} in {format} format...\")\n",
    "        \n",
    "        try:\n",
    "            # Save fact table partitioned by year and month\n",
    "            self.fact_sales \\\n",
    "                .withColumn(\"year\", substring(col(\"date_id\"), 1, 4)) \\\n",
    "                .withColumn(\"month\", substring(col(\"date_id\"), 5, 2)) \\\n",
    "                .write \\\n",
    "                .mode(\"overwrite\") \\\n",
    "                .partitionBy(\"year\", \"month\") \\\n",
    "                .option(\"path\", f\"{output_path}/fact_sales\") \\\n",
    "                .saveAsTable(\"fact_sales\")\n",
    "            \n",
    "            # Save dimension tables\n",
    "            self.dim_customers_final.write.mode(\"overwrite\").option(\"path\", f\"{output_path}/dim_customers\").saveAsTable(\"dim_customers\")\n",
    "            self.dim_products_final.write.mode(\"overwrite\").option(\"path\", f\"{output_path}/dim_products\").saveAsTable(\"dim_products\")\n",
    "            self.dim_locations_final.write.mode(\"overwrite\").option(\"path\", f\"{output_path}/dim_locations\").saveAsTable(\"dim_locations\")\n",
    "            self.dim_dates.write.mode(\"overwrite\").option(\"path\", f\"{output_path}/dim_dates\").saveAsTable(\"dim_dates\")\n",
    "            \n",
    "            logger.info(\"✓ Data saved successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving data: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def generate_summary_stats(self):\n",
    "        \"\"\"Generate comprehensive summary statistics\"\"\"\n",
    "        logger.info(\"Generating summary statistics...\")\n",
    "        \n",
    "        try:\n",
    "            # Sales summary by various dimensions\n",
    "            monthly_sales = self.fact_sales \\\n",
    "                .groupBy(substring(col(\"date_id\"), 1, 6).alias(\"year_month\")) \\\n",
    "                .agg(\n",
    "                    sum(\"revenue\").alias(\"total_revenue\"),\n",
    "                    count(\"transaction_id\").alias(\"total_transactions\"),\n",
    "                    avg(\"revenue\").alias(\"avg_transaction_value\")\n",
    "                ) \\\n",
    "                .orderBy(\"year_month\")\n",
    "            \n",
    "            category_performance = self.fact_sales \\\n",
    "                .join(self.dim_products_final, \"product_id\") \\\n",
    "                .groupBy(\"category\") \\\n",
    "                .agg(\n",
    "                    sum(\"revenue\").alias(\"total_revenue\"),\n",
    "                    count(\"transaction_id\").alias(\"total_transactions\"),\n",
    "                    avg(\"revenue\").alias(\"avg_transaction_value\")\n",
    "                ) \\\n",
    "                .orderBy(desc(\"total_revenue\"))\n",
    "            \n",
    "            regional_performance = self.fact_sales \\\n",
    "                .join(self.dim_locations_final, \"location_id\") \\\n",
    "                .groupBy(\"region\") \\\n",
    "                .agg(\n",
    "                    sum(\"revenue\").alias(\"total_revenue\"),\n",
    "                    count(\"transaction_id\").alias(\"total_transactions\")\n",
    "                ) \\\n",
    "                .orderBy(desc(\"total_revenue\"))\n",
    "            \n",
    "            logger.info(\"=== SUMMARY STATISTICS ===\")\n",
    "            logger.info(\"\\nMonthly Sales Trend (Top 10):\")\n",
    "            monthly_sales.show(10)\n",
    "            \n",
    "            logger.info(\"\\nTop Product Categories:\")\n",
    "            category_performance.show()\n",
    "            \n",
    "            logger.info(\"\\nRegional Performance:\")\n",
    "            regional_performance.show()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating summary statistics: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"Clean up resources\"\"\"\n",
    "        logger.info(\"Cleaning up resources...\")\n",
    "        self.spark.stop()\n",
    "        logger.info(\"Spark session stopped\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    from pyspark.sql import SparkSession\n",
    "    \"\"\"Main ETL pipeline execution\"\"\"\n",
    "    #etl = SalesDataETL(\"SalesAnalyticsETL\")\n",
    "\n",
    "    \n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"SalesAnalyticsETL\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.hadoop.fs.defaultFS\", \"file:///\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    etl = SalesDataETL(spark)\n",
    "    \n",
    "    try:\n",
    "        # Configuration\n",
    "        data_path = \"/Users/tariqul/Development/Python/Data transformations using PySpark/input_data\"  # Update this path\n",
    "        output_path = \"/Users/tariqul/Development/Python/Data transformations using PySpark/output_data\"  # Update this path for warehouse\n",
    "        \n",
    "        # ETL Pipeline Steps\n",
    "        logger.info(\"=== STARTING SALES ANALYTICS ETL PIPELINE ===\")\n",
    "        \n",
    "        # Step 1: Extract\n",
    "        etl.extract_data(data_path)\n",
    "        \n",
    "        # Step 2: Transform\n",
    "        etl.transform_data()\n",
    "        \n",
    "        # Step 3: Validate\n",
    "        etl.validate_data()\n",
    "        \n",
    "        # Step 4: Load (Save)\n",
    "        etl.save_data(output_path)\n",
    "        \n",
    "        # Step 5: Generate Reports\n",
    "        etl.generate_summary_stats()\n",
    "        \n",
    "        logger.info(\"=== ETL PIPELINE COMPLETED SUCCESSFULLY ===\")\n",
    "        \n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Pipeline failed: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        etl.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a96ca555-c69e-45bf-a2cc-d6111d0c104e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/10 12:38:42 WARN Utils: Your hostname, tariquls-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.0.9 instead (on interface en0)\n",
      "25/06/10 12:38:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/10 12:38:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/06/10 12:38:49 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "INFO:__main__:Spark session initialized: <pyspark.sql.session.SparkSession object at 0x1045d9c90>\n",
      "INFO:__main__:=== STARTING SALES ANALYTICS ETL PIPELINE ===\n",
      "INFO:__main__:Starting data extraction...\n",
      "INFO:__main__:Data extraction completed:                                        \n",
      "INFO:__main__:  - Sales: 100,100 records                                        \n",
      "INFO:__main__:  - Customers: 10,000 records                                     \n",
      "INFO:__main__:  - Products: 1,000 records\n",
      "INFO:__main__:  - Locations: 100 records\n",
      "INFO:__main__:Starting data transformation...\n",
      "INFO:__main__:Cleaning sales data...\n",
      "INFO:__main__:Sales data cleaned: 100,000 records after deduplication and validation\n",
      "INFO:__main__:Cleaning customer data...\n",
      "INFO:__main__:Customer data cleaned: 9,808 records                              \n",
      "INFO:__main__:Cleaning product data...\n",
      "INFO:__main__:Product data cleaned: 1,000 records\n",
      "INFO:__main__:Cleaning location data...\n",
      "INFO:__main__:Location data cleaned: 100 records\n",
      "INFO:__main__:Creating date dimension...\n",
      "INFO:__main__:Date dimension created: 1,095 records                             \n",
      "INFO:__main__:Creating fact sales table...\n",
      "INFO:__main__:Fact sales table created: 100,000 records\n",
      "INFO:__main__:Creating customer dimension...\n",
      "INFO:__main__:Customer dimension created: 9,808 records\n",
      "INFO:__main__:Creating product dimension...\n",
      "INFO:__main__:Product dimension created: 1,000 records\n",
      "INFO:__main__:Creating location dimension...\n",
      "INFO:__main__:Location dimension created: 100 records\n",
      "INFO:__main__:Data transformation completed successfully\n",
      "INFO:__main__:Starting data validation...\n",
      "INFO:__main__:=== DATA VALIDATION SUMMARY ===                                   \n",
      "INFO:__main__:Total fact records: 100,000\n",
      "INFO:__main__:Null foreign keys: 0\n",
      "INFO:__main__:Business rule violations: 0\n",
      "INFO:__main__:Referential integrity issues: 1859\n",
      "INFO:__main__:Date range: 20220101 to 20241230\n",
      "INFO:__main__:✓ All critical validations passed\n",
      "INFO:__main__:Saving data to /Users/tariqul/Development/Python/Data transformations using PySpark/output_data in parquet format...\n",
      "25/06/10 12:39:51 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "25/06/10 12:39:53 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "25/06/10 12:39:53 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "25/06/10 12:39:54 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "25/06/10 12:39:55 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "25/06/10 12:39:56 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "25/06/10 12:39:57 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "25/06/10 12:39:58 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "25/06/10 12:40:05 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "25/06/10 12:40:06 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "INFO:__main__:✓ Data saved successfully                                         \n",
      "INFO:__main__:Generating summary statistics...\n",
      "INFO:__main__:=== SUMMARY STATISTICS ===\n",
      "INFO:__main__:\n",
      "Monthly Sales Trend (Top 10):\n",
      "INFO:__main__:                                                                  \n",
      "Top Product Categories:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+---------------------+\n",
      "|year_month|     total_revenue|total_transactions|avg_transaction_value|\n",
      "+----------+------------------+------------------+---------------------+\n",
      "|    202201|        1810908.38|              2745|    659.7116138433515|\n",
      "|    202202|1728242.4099999997|              2519|    686.0827352123857|\n",
      "|    202203|         1939811.6|              2890|    671.2150865051904|\n",
      "|    202204|1842994.4199999995|              2746|    671.1560160233064|\n",
      "|    202205|1910003.5300000014|              2807|    680.4430103313151|\n",
      "|    202206|1834977.8999999994|              2677|    685.4605528576762|\n",
      "|    202207|1905808.2000000002|              2791|    682.8406305983519|\n",
      "|    202208| 1937517.629999999|              2829|    684.8772110286317|\n",
      "|    202209|1864006.0200000005|              2705|    689.0964953789281|\n",
      "|    202210|1947986.1400000001|              2832|    687.8482132768362|\n",
      "+----------+------------------+------------------+---------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:                                                                  \n",
      "Regional Performance:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+------------------+---------------------+\n",
      "|     category|    total_revenue|total_transactions|avg_transaction_value|\n",
      "+-------------+-----------------+------------------+---------------------+\n",
      "|   Automotive|9789641.330000019|             15205|    643.8435600131548|\n",
      "|Home & Garden|9603142.490000024|             14288|     672.112436310192|\n",
      "|        Books|9176482.970000006|             13425|    683.5369065176914|\n",
      "|     Clothing|9001828.860000018|             12657|    711.2134676463631|\n",
      "|       Beauty|8135679.610000008|             12050|    675.1601336099592|\n",
      "|  Electronics|8089031.240000006|             11487|     704.190061809002|\n",
      "|       Sports|7768540.860000004|             11197|    693.8055604179694|\n",
      "|         Toys|6558976.750000004|              9691|    676.8111392013212|\n",
      "+-------------+-----------------+------------------+---------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:=== ETL PIPELINE COMPLETED SUCCESSFULLY ===\n",
      "INFO:__main__:Cleaning up resources...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------------------+\n",
      "| region|       total_revenue|total_transactions|\n",
      "+-------+--------------------+------------------+\n",
      "|  South|1.8183891300000045E7|             26775|\n",
      "|Central|1.7634833179999985E7|             25902|\n",
      "|  North| 1.565890121000002E7|             22980|\n",
      "|   West|1.1812267070000019E7|             17222|\n",
      "|   East|   4833431.350000001|              7121|\n",
      "+-------+--------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Spark session stopped\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b8c876-9ac2-4a1c-92ed-36fb2eed72be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
